{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PimaIndians Diabetes Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the latest TransmogrifAI Core jar version 0.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%classpath add mvn com.salesforce.transmogrifai transmogrifai-core_2.11 0.6.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the Spark MLlib version 2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e8b9a15-0ab9-41d2-8781-99766daca92e",
       "version_major": 2,
       "version_minor": 0
      },
      "method": "display_data"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%classpath add mvn org.apache.spark spark-mllib_2.11 2.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the scala packages and classes \n",
    "Let us import the relevant classes for for Spark and TransmogrifAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.SparkConf\n",
    "import org.apache.spark.sql.SparkSession\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.sql.functions.udf\n",
    "\n",
    "import com.salesforce.op._\n",
    "import com.salesforce.op.features._\n",
    "import com.salesforce.op.features.types._\n",
    "import com.salesforce.op.evaluators.Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.OpWorkflow\n",
    "import com.salesforce.op.evaluators.Evaluators\n",
    "import com.salesforce.op.readers.DataReaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate Spark instance and make it implicit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val conf = new SparkConf().setMaster(\"local[*]\").setAppName(\"PimaIndiansClassification\")\n",
    "implicit val spark = SparkSession.builder.config(conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema class\n",
    "Create the schema class with dependent and to be predicted variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class PimaIndians\n",
    "(\n",
    "  numberOfTimesPreg: Double,\n",
    "  plasmaGlucose: Double,\n",
    "  bp: Double,\n",
    "  spinThickness: Double,\n",
    "  serumInsulin: Double,\n",
    "  bmi: Double,\n",
    "  diabetesPredigree : Double,\n",
    "  ageInYrs : Double,\n",
    "  piClass: String\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Creation\n",
    "Create the feature from schema class and specify as Predictor or Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val numberOfTimesPreg = FeatureBuilder.Real[PimaIndians].extract(_.numberOfTimesPreg.toReal).asPredictor\n",
    "val plasmaGlucose = FeatureBuilder.Real[PimaIndians].extract(_.plasmaGlucose.toReal).asPredictor\n",
    "val bp = FeatureBuilder.Real[PimaIndians].extract(_.bp.toReal).asPredictor\n",
    "val spinThickness = FeatureBuilder.Real[PimaIndians].extract(_.spinThickness.toReal).asPredictor\n",
    "val serumInsulin = FeatureBuilder.Real[PimaIndians].extract(_.serumInsulin.toReal).asPredictor\n",
    "val bmi = FeatureBuilder.Real[PimaIndians].extract(_.bmi.toReal).asPredictor\n",
    "val diabetesPredigree = FeatureBuilder.Real[PimaIndians].extract(_.diabetesPredigree.toReal).asPredictor\n",
    "val ageInYrs = FeatureBuilder.Real[PimaIndians].extract(_.diabetesPredigree.toReal).asPredictor\n",
    "val piClass = FeatureBuilder.Text[PimaIndians].extract(_.piClass.toText).asResponse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Data path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " val trainFilePath = \"../src/main/resources/PimaIndiansDataset/primaindiansdiabetes.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.features.types._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the trainDataReader which is a csv reader."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Factory method to create an instance of Data Reader for CSV data. Each CSV record will be automatically converted to an Avro record using the provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spark.implicits._ \n",
    "\n",
    "val trainDataReader = DataReaders.Simple.csvCase[PimaIndians](\n",
    "      path = Option(trainFilePath)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Created the features sequence and call `.transmogrify()` on it. Create a DataSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.stages.impl.tuning.{DataCutter, DataSplitter}\n",
    "\n",
    "val features = Seq( numberOfTimesPreg, plasmaGlucose,bp,spinThickness,serumInsulin,\n",
    "    bmi,diabetesPredigree,ageInYrs).transmogrify()\n",
    "val randomSeed = 42L\n",
    "val splitter = DataSplitter(seed = randomSeed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Encoder for serializing Java Bean of type `PimaIndians`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.Encoders\n",
    "\n",
    "implicit val piEncoder = Encoders.product[PimaIndians]\n",
    "//val piReader = DataReaders.Simple.csvCase[PimaIndians]()\n",
    "val labels = piClass.indexed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imprt MultiClassificationSelector with the splitter addded and inputs set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import com.salesforce.op.stages.impl.classification.MultiClassificationModelSelector\n",
    "import com.salesforce.op.stages.impl.tuning.DataCutter\n",
    "\n",
    "val cutter = DataCutter(reserveTestFraction = 0.2, seed = randomSeed)\n",
    "val prediction = MultiClassificationModelSelector\n",
    "    .withCrossValidation(splitter = Option(cutter), seed = randomSeed)\n",
    "    .setInput(labels, features).getOutput()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an Evaluator of type MultiClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val evaluator = Evaluators.MultiClassification.f1().setLabelCol(labels).setPredictionCol(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow\n",
    "\n",
    "Once all the Features and Feature transformations have been defined, actual data can be materialized by adding the desired Features to a TransmogrifAI Workflow and feeding it a DataReader. When the Workflow is trained, it infers the entire DAG of Features, Transformers, and Estimators that are needed to materialize the result Features. It then prepares this DAG by passing the data specified by the DataReader through the DAG and fitting all the intermediate Estimators in the DAG to Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val workflow = new OpWorkflow().setResultFeatures(prediction, labels).setReader(trainDataReader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the workflow\n",
    "When a workflow gets fitted a number of things happen: the data is read using the DataReader, raw Features are built, each Stage is executed in sequence and all Features are materialized and added to the underlying Dataframe. During Stage execution, each Estimator gets fitted and becomes a Transformer. A fitted Workflow (eg. a OpWorkflowModel) therefore contains sequence of Transformers (map operations) which can be applied to any input data of the appropriate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val workflowModel = workflow.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score and evaluate the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val (scores, metrics)  = workflowModel.scoreAndEvaluate(evaluator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting ModelInsights from a Fitted Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val modelInsights = workflowModel.modelInsights(prediction)\n",
    "val labelSummary = modelInsights.label\n",
    "println(\"labelName: \" + labelSummary.labelName)\n",
    "println(\"rawFeatureName: \"+ labelSummary.rawFeatureName)\n",
    "println(\"stagesApplied: \" + labelSummary.stagesApplied)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the model features from modelInsights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val modelFeatures = modelInsights.features.flatMap( feature => feature.derivedFeatures)\n",
    "modelFeatures.foreach(x => println(x.derivedFeatureName + \", \" + x.contribution(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val featureContributions = modelFeatures.map( feature => (feature.derivedFeatureName,\n",
    "  feature.contribution.map( contribution => math.abs(contribution))\n",
    "  .foldLeft(0.0) { (max, contribution) => math.max(max, contribution)}))\n",
    "\n",
    "val sortedContributions = featureContributions.sortBy( contribution => -contribution._2)\n",
    "sortedContributions.foreach(x => println(x._1 + \",\" + x._2) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "",
   "name": "Scala",
   "nbconverter_exporter": "",
   "version": "2.11.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
